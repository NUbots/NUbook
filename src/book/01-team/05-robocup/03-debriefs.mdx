---
section: Team
chapter: RoboCup
title: Debriefs
description: Debriefs and takeaways from our performances at RoboCup.
slug: /team/robocup/debriefs
---

## 2020: France, Bordeaux

Due to COVID-19 the 2020 competition in France, Bordeaux was cancelled.

## 2019: Sydney, Australia

### What went wrong

#### Walking

- We were using a legacy walk engine (open loop with no reactive balance control, designed for smaller robots (Darwins))
- The simple walk path planner was previously not configured for iGus and the planner did not tell the robot to go to the correct position when it could not see the ball

#### Vision

- Wasn’t properly implemented until the second day of the competition.
- There was no integration testing with vision and the rest of the system.
- Green on the field wasn’t detected properly due to low saturation in the images.
- Seg faults were found due to edge conditions with the goal post detection which caused instability/restarts in the walk
- Visual Mesh
  - Couldn’t detect field lines because we couldn’t detect the field (wrong green color, network didn’t have the right training data).
  - Couldn’t see the goals though because they were thinner than what was in the training data.
  - Visual Mesh packets to NUsight were too big, there was lots of dropped packets.

#### Odometry

- Since moving to the CM740 the axis of the gyroscope and accelerometer changed. This change resulted in uncovering an error in the Kalman Filter, which was corrected during the competition.
- Foot down detection was not accurate
  - neural network training was inadequate to determine foot-downness. Tuning of the walk was performed after training the network, so the trained models were not accurate. The walk was inadequate for getting good network training data as the robots weren’t lifting their feet off the ground enough to get good data - there were lots of noise in the small steps.

#### Mechanical/Hardware issues

- Problems with servos overloading and locking, possibly due to motor drive shafts bending, or motors magnetically locking to one set.
- Head fell off due to not being bolted in correctly.
- Studs fell off/got stuck in the turf, possibly due to lack of adequate threading and inappropriate studs for our usage.
- Robot disconnected from power supply when it fell over.

#### Configuration issues

- Multiple layers of configuration (with multiple platforms, robot-specific configurations, and multiple configuration files for each system) led to confusion and made it difficult to tune the walk, vision camera parameters, etc.
- Changing network configurations for different fields could be made faster/easier - perhaps a script that takes params and updates the network configuration correctly

#### Scripts

- Needed re-tuning because: different fields (turf), different robots (scripts weren’t tuned before RoboCup for the new robots).

#### NUsight2

- Chart view rendering was slow (especially when running `start/dev`)
- Network issues with NUsight when trying to run the robots with a static, direct ethernet connection.

#### RoboCup competition and games

- Team members didn’t go to see most of the games, was demotivating. Robot handling/computer operation during games should be shared or rostered - every team member should be trained on how to do this, to share the load. This year it was tough having just one/two people do it.
- Wrong prioritization of tasks, could have focused more on immediately important things.
- Need a pre-RoboCup briefing/training for team members
  - Handling robots, during games - putting them on the field, when/where to touch, place remove robots, what to say to refs, etc.
  - How to fix robots correctly.
  - How the configuration system works - how to adjust and tune parameters, NUgus specific vs global config, `/etc/network/interfaces`, robocup services (`systemctl`) enable disable start stop, `globalconfig.yaml`, `soccerstrategy.yaml`, team ID and player ID.
- Post RoboCup de-briefing with a physical debriefing meeting should be done.
- Should write an entry in the team handbook about what went wrong, what was done well, what to do for next RoboCup.

### What went well

- NUsight2 was very useful for debugging the odometry filter, less problems than NUsight 1. Had the NUgus models which was good.
- Code errors in odometry problem were fixed, but it still needs tuning.
- Visual Mesh could successfully find the balls when the green horizon was detected on the field.
- Connected to Game Controller most of the time.
- Getup scripts were good - didn’t work when the robot had on the game vest - it couldn’t turn itself over.
- Penalty kicks were good (won 3:0) - did a good job of tuning them well - although they did some damage to the robots.
- New members participated more, learned a lot.
- Recorded and labelled a lot of data which could be used to fix vision and walk issues next year:
  - Recorded sensors (motor positions), gyroscope and IMU data
  - Recorded vision data with visual mesh classification
  - Started work on creating a HDR module for creating HDR images from the robot’s camera - useful for getting more image data with different exposures to train vision. Module needs to be finished
  - Annotated segmentation maps for images obtained from the robot
  - Recorded 360-degree HDR images of the adult sized field
  - Recorded during robocup/during games
  - Recordings are NBS files saved on Trent’s laptop, to be stored on the NAS.
- Starting writing team handbook and more comprehensive documentation

### What’s next

- Better testing for next year.
- Finish up initial documentation/team handbook.
- Finish HDR image creation module.
- Re-do the shoulder and hips to be more rigid - so they don’t flex when loaded. Need more rigid connection between body and hip joints.
- Implement new battery system (power tool batteries).
- Finish NUSense (code and hardware changes).
- Finish up the script tuner in NUsight2 for better tuning of scripts.
- Vision
  - Implement field lines detection
  - Collect more diverse/real training training data (from cameras) to train/tune the vision system. More data with less synthetic features in the dataset.
  - Tune/improve of ball and goal detection
  - Visual Mesh training for visual mesh (needs more training data from real robot, varied saturations, hue, exposure, gain.)
- Get a better walk
