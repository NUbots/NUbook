---
section: Guides
chapter: Main Codebase
title: Maintaining Subsystems
description: How to maintain subsystems within the main codebase.
slug: /guides/main/maintaining-subsystems
---

This page details how to maintain various subsystems within the main codebase.

## Vision

Before vision can work on the robot, odometry and kinematics must work on the robot. If these are inaccurate, then the green horizon is inaccurate.

### Dataset Generation

Synthetic and semi-synthetic training data for vision can be generated using NUpbr. Pre-generated datasets for training the Visual Mesh can be found on the <abbr title="Network-Attached Storage">NAS</abbr> in the lab.

#### NUpbr

NUpbr is a Physically Based Rendering tool created in Blender. It creates semi-synthetic images with corresponding segmentation images for training.

Find out how to get and use NUpbr on the [NUpbr NUbook page](/system/tools/nupbr) and the [NUpbr GitHub repository](https://github.com/NUbots/NUpbr).

#### Setting Up The Data

The [Visual Mesh repository](https://github.com/Fastcode/VisualMesh) contains information on how to set up the datasets for training. It contains [a script](https://github.com/Fastcode/VisualMesh/blob/master/training/build_dataset.py) that takes data and converts it into the tfrecord format that the Visual Mesh expects. The script requires a folder of raw images, segmentation masks, and metadata. NUpbr will provide all these as output, and such data can be found on the NAS.

### The Visual Mesh

#### Training and Testing

Go to the [NUbook Visual Mesh Getting Started guide](/guides/tools/visualmesh) to find out how to train and test a network, with an example dummy dataset.

#### Exporting Configuration

The resulting network should be exported to a [yaml](https://yaml.org/) file and added to the NUbots codebase, by completing the following steps.

Example yaml files can be found [in the Visual Mesh repository](https://github.com/Fastcode/VisualMesh/blob/master/example/model.yaml) and [in the NUbots repository](https://github.com/NUbots/NUbots/blob/master/module/vision/VisualMesh/data/config/VisualMesh.yaml). Assuming such a file is in the output directory, run

```bash
./mesh.py export <output_dir>
```

to export the weights to a file in the directory `<output_dir>`.

Replace the NUbots repository `VisualMesh.yaml` configuration with this configuration file. View the [Git Guide](/guides/general/git) for information on using Git and submitting this change in a pull request.

### Camera Calibration

The vision system cannot work optimally if the cameras are not calibrated correctly. [The input page](/system/subsystems/input) describes the camera parameters that can be calibrated.

An automatic camera calibration tool is available in the NUbots repository. See the [camera calibration guide](guides/main/camera-calibration#page-content) to find out how to use this tool.

### Testing

After updating the Visual Mesh in the NUbots repository, it should be tested before merging.

1. Build the code, ensuring `ROLE_visualmesh` is set to `ON` in `./b configure -i`, then install it to the robot. See the [Getting Started page](/guides/main/getting-started) for more information about this step. Ensure the new configuration file has been installed. Check out the [Build System page](/system/foundations/build-system#install) to find out more about options when installing onto the robot.

2. When your new Visual Mesh is installed onto the robot, connect to the robot using

   ```bash
   ssh nubots@<address>
   ```

3. Ensure NUsight is on.

   ```bash
   nano config/NetworkForwarder.yaml
   ```

   Turn `vision object` and `compressed images` on. Run NUsight using `yarn prod` and navigate to the NUsight page in your browser. More on NUsight can be found on [the NUsight NUbook page](/system/tools/nusight).

4. Run the visualmesh role

   ```bash
   ./visualmesh
   ```

5. Wait for the cameras to load and then watch the Vision tab in NUsight. To determine if the output is correct, consult the [vision page](/system/subsystems/vision) for images of the expected output.

If you would like to see the Visual Mesh output in NUsight, you will need to log the data and run it back in NUsight using DataPlayback, since the data is too large to send over a network. See the [DataLogging and DataPlayback guide](/guides/main/data-recording-playback), considering the following:

- Use the [`visualmesh`](https://github.com/NUbots/NUbots/blob/master/roles/visualmesh.role) role to record the data.
- Set `message.output.CompressedImage` to `true` and add `message.vision.VisualMesh: true` in both [`DataLogging.yaml](https://github.com/NUbots/NUbots/blob/master/module/support/logging/DataLogging/data/config/DataLogging.yaml) and [`DataPlayback.yaml](https://github.com/NUbots/NUbots/blob/master/module/support/logging/DataPlayback/data/config/DataPlayback.yaml).
- Use the [`playback`](https://github.com/NUbots/NUbots/blob/master/roles/playback.role) role to playback the data, without changes.

### Tuning Detectors

If the Visual Mesh trained with good results but when testing in the system the network did not appear to work very well, you may need to tune the detectors.

[BallDetector.yaml](https://github.com/NUbots/NUbots/blob/master/module/vision/BallDetector/data/config/BallDetector.yaml) and [GoalDetector.yaml](https://github.com/NUbots/NUbots/blob/master/module/vision/GoalDetector/data/config/GoalDetector.yaml) contain the values for tuning the ball and goal detectors respectively.

1. Build and install the `visualmesh` role to a robot.
2. SSH onto the robot.
3. Enable NUsight messages on the robot by running

   ```bash
   nano config/NetworkForwarder.yaml
   ```

   and set `message.vision.Balls` and `message.vision.Goals` to `true`. This command uses the nano text editor, see this [cheatsheet](https://www.nano-editor.org/dist/latest/cheatsheet.html) for common commands.

4. Run NUsight using `yarn prod` on a computer. Set up NUsight using the [Getting Started page](/guides/main/getting-started) if necessary.
5. Run `./visualmesh` on the robot.
6. Alter the configuration file for the detectors while simultaneously running the binary on the robot. In a new terminal, SSH onto the robot again and run:

   ```bash
   nano config/BallDetector.yaml
   ```

   Change the values and upon saving, the changes will be used immediately by the robot **without** needing to rebuild or rerun the `./visualmesh` binary.

7. Repeat #6 for the goal detector by running

   ```bash
   nano config/GoalDetector.yaml
   ```

In general, it might be useful to adjust the `confidence_threshold` on both detectors to improve the results. Other variables may give better results with different values, except for `log_level` and the covariances (`goal_projection_covariance` and `ball_angular_cov`).
