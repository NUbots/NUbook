---
chapter: RoboCup Debriefs
title: Sydney 2019
description: Debrief after 2019 RoboCup in Sydney, Australia.
slug: /debrief-2019
---

## What went wrong

### Walking

- We were using a legacy walk engine (open loop with no reactive balance control, designed for smaller robots (Darwins))

- The simple walk path planner was previously not configured for iGus and the planner did not tell the robot to go to the correct position when it could not see the ball
	
- CM740 issues - Since moving to the CM740 the axis of the gyroscope and accelerometer changed. This change meant the Kalman filter had an error exposed.

### Vision

- Wasn’t actually implemented until the second day of the competition. 
- There was no integration testing with vision and the rest of the system.
- Green on the field wasn’t detected properly due to low saturation in the images
- Seg faults were found when the goal posts were too small which caused instability/restarts in the walk

### Visual Mesh

- Couldn’t detect field lines because we couldn’t detect the field (wrong green color, network didn’t have the right training data). 

- Couldn’t see the goals though because they were thinner than what was in the training data.

- Visual Mesh packets to NUsight were too big, there was lots of dropped packets.

- Configuration issues - multiple layers of configuration (with multiple platforms, robot-specific configurations, and multiple configuration files for each system) led to confusion and made it difficult to tune the walk, vision camera parameters, etc
Changing network configurations for different fields could be made faster/easier - perhaps a script that takes params and updates the network configuration correctly

### Mechanical/Hardware issues

- Problems with servos: overloading, locking, falling off (head not bolted in correctly fell off) - possible reasons: motor drive shafts bending, or motors magnetically locking to one set
- Studs fell off/got stuck in the turf (lack of adequate threading, inappropriate studs for our usage)
- When the robot fell it disconnected from power supply
Odometry - problems with the odometry/kalman filter - code was wrong - eventually fixed, see next section

### Foot Down Detection 

- neural network training was inadequate to determine foot-downness (reasons: tuning of the walk was performed after training the network, so the trained models were not accurate; the walk was inadequate for getting good network training data as the robots weren’t lifting their feet off the ground enough to get good data - there were lots of noise in the small steps).

### Scripts

- Needed re-tuning because: different fields (turf), different robots (scripts weren’t tuned before RoboCup for the new robots)
Localisation - no comments as we didn’t get around to testing/using it due to our other issues.

### NUsight2
- Chart view rendering was slow (especially when running `start/dev`)
- Network issues with NUsight when trying to run the robots with a static, direct ethernet connection.

### RoboCup competition and games
- Team members didn’t go to see most of the games, was demotivating. Robot handling/computer operation during games should be shared or rostered - every team member should be trained on how to do this, to share the load. This year it was tough having just one/two people do it (mostly Matt).
- Wrong prioritization of tasks, could have focused more on immediately important things.
- Need a pre-RoboCup briefing/training for team members
- Handling robots, during games - putting them on the field, when/where to touch, place remove robots, what to say to refs, etc.
- How the config system works - how to adjust and tune parameters, igus specific vs global config, /etc/network/interfaces, robocup services (systemctl) enable disable start stop, globalconfig.yaml, soccerstrategy.yaml, team ID and player ID.  
- Post RoboCup de-briefing
- Writing an entry in the team handbook about (What went wrong, what was done well, what to do for next RoboCup)
- Physically hold a debriefing meeting

## What went well
- NUsight2 was very useful for debugging the odometry filter, less problems than NUsight 1. Had the iGus models which was good.
- Code errors in odometry problem were fixed, but it still needs tuning.
- Visual Mesh could successfully find the balls when the green horizon was detected on the field.
- Connected to Game Controller most of the time.
- Getup scripts were good - didn’t work when the robot had on the game vest - it couldn’t turn itself turn over.
- Penalty kicks were good (won 3:0) - did a good job of tuning them well - although they did some damage to the robots.
- New members participated more, learned a lot.
- Recorded and labelled a lot of data which could be used to fix vision and walk issues next year:
- Recorded sensors (motor positions), gyroscope and IMU data
- Recorded vision data with visual mesh classification
- Started work on creating a HDR module for creating HDR images from the robot’s camera - useful for getting more image data with different exposures to train vision. Module needs to be finished
- Annotated segmentation maps for images obtained from the robot
- Recorded 360-degree HDR images of the adult sized field
- Recorded during robocup/during games
- Recordings are NBS files saved on Trent’s laptop, to be stored on the NAS.
- Starting writing team handbook and more comprehensive documentation

## What’s next

- TESTING!!!
- Finish up initial documentation/team handbook
- Finish HDR image creation module
- Re-do the shoulder and hips to be more rigid - so they don’t flex when loaded. Need more rigid connection between body and hip joints… I’m getting there (taylor)
- Implement new battery system (Power tool batteries)
- Finish NUSense (Code and Hardware changes)
- Finish up the script tuner in NUsight2 for better tuning of scripts.
- Implement field lines detection
- Collect more diverse/real training training data (from cameras) to train/tune the vision system. More data with less synthetic features in the dataset.
- Tune/improve of ball and goal detection
visual mesh training for visual mesh (needs more training data from real robot, varied saturations, hue, exposure, gain.) 
- Get a better walk

