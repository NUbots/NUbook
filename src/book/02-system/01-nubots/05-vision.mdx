---
section: System
chapter: NUbots
title: Vision
description: Introduction to vision and how it works in the NUbots codebase.
slug: /system/nubots/vision
---

The vision system aims to convert images from a camera in the robot into useful information, such as ball position, goal position, field lines and obstacles. This information is used to determine behaviour and localisation. The vision system encompasses the [hardware](/system/hardware/overview/#cameras), object detection, and post processing.

# Visual Mesh

The [Visual Mesh](https://doi.org/10.1007/978-3-030-27544-0_4) underpins the vision system, and is used for sparse detection of balls, points on the field, field lines, goal posts and other robots.

The Visual Mesh is a convolutional neural network for object detection based on a mesh which samples pixels from an image. The mesh is depth independent and the number of mesh points is small enough to allow a CNN to run in real-time on a humanoid robot. Tests on the CPU from the Intel NUC7i7BNH show that a nine layer Visual Mesh has an execution time of 2.44ms. Furthermore, the Visual Mesh does not degrade in accuracy when objects occur at different distances. This is due to the depth independence introduced by using the mesh. The Visual Mesh has been used to detect a soccer ball on a field up to 10m away from the camera. The Visual Mesh can detect soccer balls, field lines and goal posts, however it is built assuming the object to be detected is spherical.

The Visual Mesh is created using the following geometric assumptions for each image:

- The camera lens type, height and orientation with respect to the ground are known.
- The object is spherical with known radius and remains on the ground.

The height, orientation and radius are used to create a set of unit vectors based at the camera position. Any vector with its origin at the camera can be thought of as a ray of light travelling towards the camera. If the vector is within the field of view of the camera it will be collected by the lens. The vector is mapped to a point on the image using the lens projection equation, provided the point is within the bounds of the image sensor of the camera. The Visual Mesh is formed by taking an array of vectors and associating each of them with a point and pixel in the image. The array of vectors is specially constructed using two equations to efficiently sample the space around the camera for the object. This means the points (sample points) on the image will efficiently sample the image for the object.

Because the vectors are generated in the space around the camera, the sample is consistent despite changes in lens type, image resolution, and the apparent size of an object due to distance. Each sample point is connected to its six closest neighbours and these connections become the edges of the mesh. A fully [convolutional neural network](https://doi.org/10.1109/TPAMI.2016.2572683) is used on the mesh where each sample point and it's neighbours become the input to a convolution. All layers in the network use seven point convolutions since there are six neighbours for each sample point. A parameter controls the number of sample points on the object which determines the level of detail available to the network.

The height and orientation of the camera is tracked using the kinematics and inertial measurement unit of the robot. The radius of the soccer ball is given. The unit vectors only need to be calculated once for each height and radius pair. As the robot walks its height varies. A series of Visual Meshes for different heights are calculated on startup. For each image this information is recorded and the appropriate Visual Mesh is chosen. A binary search pattern is used to select the vectors that will become points on the image based on the camera's orientation. The partitions of the binary search pattern are built on startup and the search is run in real-time. Every training and run-time image is converted into the mesh before the network is run. The network labels each point of the mesh with the probability it belongs to an object class. A post-processing algorithm based on the probabilities of the mesh can cluster the sample points to determine an object's position.

# Post Processing

From the Visual Mesh a series of specialised detectors are employed to detect field edges, balls, and goal posts. Firstly, all points that the Visual Mesh has identified as either field points or field line points are clustered into connected regions and, each cluster is then either merged or discarded using some heuristics until a single cluster remains. Finally, an upper convex hull algorithm is applied to the final cluster determine the edge of the field.

The ball detector forms clusters out of all the points that the Visual Mesh has identified as ball points. Specifically, the clusters are formed from Visual Mesh points that are identified as being a ball point, but have at least $1$ neighbour which is **not** a ball point. This allows us to form clusters of ball edge points. Any clusters which are not below the field edge are discarded. A circular cone is then fitted to each cluster. The cone axis is determined from the line segment between the centre on the camera and the average of all ball edge points. The radius of the cone is determined by the maximum distance between the ball edge points and the average of all of the ball edge points. Different heuristics, such as degree of circle fit and different distance metrics, are then used to discard to cones.

The goal post detector follows a similar structure to the ball detector. Clusters are formed from goal post edge points and any clusters that do not intersect the field edge are discarded. The bottom centre point of the goal post is then found by averaging the edge points. The distance to the goal posts are determined and if there are multiple goal posts detected an attempt is made to assign **leftness** and **rightness** to each post.

At present, a field line detector is not implemented, but will likely be implemented before the next competition.

All calculations in all detectors are done in 3D world coordinates.
