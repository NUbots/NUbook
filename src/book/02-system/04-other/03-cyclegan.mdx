---
section: System
chapter: Other Projects
title: CycleGAN
description: A CycleGAN implementation for dataset generation.
slug: /system/cyclegan
---

The NUbots robots must be able to detect object in their environments so they are able to play soccer. We use machine learning to detect where objects are in the world. To find out more about our vision system, head over to [the vision page](/system/vision).

The problem with machine learning is that you need data to train on. Rather than take thousands of pictures and annotate them by hand, we use advanced techniques to generate the data. [NUpbr](/system/nupbr) is used to generate semi-synthetic images with the corresponding segmentation masks. These datasets are limited by their synthetic nature. To try to bridge this gap, we use CycleGAN to generate realistic images from NUpbr images.

## GANs

A [Generative Adversarial Network (GAN)](https://arxiv.org/abs/1406.2661) is a machine learning technique developed in 2014 which aims to create new data based on the given training data. A GAN consists of a generator and a discriminator. The generator creates fake data and the discriminator classifies the data as real or fake.

A [Cycle-Consistent Adversarial Network (CycleGAN)](https://arxiv.org/abs/1703.10593) performs unpaired image-to-image translation. CycleGAN is based on [image-to-image translation GAN (pix2pix)](https://arxiv.org/abs/1611.07004). The main difference between these two GAN models is that CycleGAN uses unpaired data while pix2pix uses paired data. CycleGANs translate some domain A to another domain B, and vice versa. It trains not only by using A and B as input to the A and B generators respectively, but it also uses the output of each generator as the input for the other generator. This is the cycle input. The CycleGAN also uses an identity mapping to stabilise the training. It will input a real A image into the B generator, which should give back the original real A input. This is also done for real B images into the A generator.

## The Model

This implementation of CycleGAN uses real images and NUpbr paired raw and segmentation images. The NUpbr raw images represent the A domain of the CycleGAN. The real images represent the B domain of the CycleGAN. We train the model on the fake images and NUpbr segmentation images to obtain a dataset of pseudo-real images.

For a given real NUpbr raw image, we will also load in the corresponding real NUpbr segmentation image into the model. We set the non-black (not background) pixels of the segmentation image to white, and ensure the black is true black. This gives us a black and white image. The black portions of the image represent '0' and the white portions of the image represent '1'. This is called our attention mask.

After we get a fake real image, we use the attention mask to add back in the original background.

```py
masked_fake_B = fake_B*attention + real_A*(1-attention)
```

`fake_B*attention` will give us the non-background pixels of the fake real image. `real_A*(1-attention)` will give us the background pixels of the original real NUbpr image. Adding these both together, we get the fake non-background elements and the real background elements in one image. This result is then used for training. This removes the consideration of the background from the training, and instead lets the generator focus its attention on the non-background elements.

Since we do not have an attention mask for images in the B domain, we focus more on cycling on the original real A image. From a real B image, we generate a fake A image. We do not cycle from here. From a real A image, we cycle on the resulting fake B image, and then again on the resulting cycled fake A image. All the cycles from the real A image can use the attention mask from the corresponding segmentation mask for A.
